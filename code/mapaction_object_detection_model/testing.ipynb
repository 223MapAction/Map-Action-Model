{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from map_action_tranform import get_transform\n",
    "from map_action_data_loader import map_action_test_data_loader, map_action_test_dataset \n",
    "from m_a_detection_model import map_action_instance_segmentation_model\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_score, accuracy_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate IoU\n",
    "def calculate_iou(pred_boxes, true_boxes):\n",
    "    x1 = torch.max(pred_boxes[:, 0], true_boxes[:, 0])\n",
    "    y1 = torch.max(pred_boxes[:, 1], true_boxes[:, 1])\n",
    "    x2 = torch.min(pred_boxes[:, 2], true_boxes[:, 2])\n",
    "    y2 = torch.min(pred_boxes[:, 3], true_boxes[:, 3])\n",
    "\n",
    "    intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "    area_pred = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    area_true = (true_boxes[:, 2] - true_boxes[:, 0]) * (true_boxes[:, 3] - true_boxes[:, 1])\n",
    "    union = area_pred + area_true - intersection\n",
    "\n",
    "    iou = intersection / union\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing pipeline\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    for images, targets in test_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        for output, target in zip(outputs, targets):\n",
    "            pred_boxes = output['boxes'].cpu()\n",
    "            true_boxes = target['boxes'].cpu()\n",
    "            all_pred_boxes.append(pred_boxes)\n",
    "            all_true_boxes.append(true_boxes)\n",
    "\n",
    "    all_pred_boxes = torch.cat(all_pred_boxes, dim=0)\n",
    "    all_true_boxes = torch.cat(all_true_boxes, dim=0)\n",
    "\n",
    "    ious = calculate_iou(all_pred_boxes, all_true_boxes)\n",
    "\n",
    "    # Threshold for considering a detection correct\n",
    "    detection_threshold = 0.5\n",
    "    pred_labels = (ious > detection_threshold).float()\n",
    "\n",
    "    # Convert ious to binary predictions\n",
    "    binary_ious = (ious > detection_threshold).float()\n",
    "\n",
    "    # Calculate average precision using binary predictions\n",
    "    \n",
    "    average_precision = average_precision_score(np.ones(len(binary_ious)), binary_ious) \n",
    "\n",
    "    true_labels = torch.ones(len(all_true_boxes))\n",
    "    \n",
    "    # ...\n",
    "    true_labels = torch.ones(len(all_true_boxes))\n",
    "    print(f'Length of true_labels: {len(true_labels)}')\n",
    "    print(f'Length of pred_labels: {len(pred_labels)}')\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    # ...\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    \n",
    "    print(f'Mean Average Precision (mAP): {average_precision:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "\n",
    "    # Plotting precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(true_labels, ious)\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 11]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m test_model(model, test_loader, device)\n",
      "\u001b[1;32m/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m average_precision \u001b[39m=\u001b[39m average_precision_score(np\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(binary_ious)), binary_ious) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m true_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(all_true_boxes))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(true_labels, pred_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m precision \u001b[39m=\u001b[39m precision_score(true_labels, pred_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mapaction/mapaction_env/Map-Action-Model/code/mapaction_object_detection_model/testing.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMean Average Precision (mAP): \u001b[39m\u001b[39m{\u001b[39;00maverage_precision\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 11]"
     ]
    }
   ],
   "source": [
    "root_dir_test = '/home/mapaction/Documents/Exp-data/Coco/images/'\n",
    "annotation_file_test = '/home/mapaction/Documents/Exp-data/Coco/result.json'\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = '/home/mapaction/mapaction_env/Map-Action-Model/model/MAISM1.pth'\n",
    "model = map_action_instance_segmentation_model(2)  # Initialize your model architecture\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Create the testing dataset\n",
    "test_transform = get_transform\n",
    "test_dataset = map_action_test_dataset\n",
    "  # Replace with your class names\n",
    "\n",
    "# Create a data loader for testing\n",
    "test_loader = map_action_test_data_loader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "test_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or use the metrics as needed\n",
    "print(f'mAP: {mAP:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
